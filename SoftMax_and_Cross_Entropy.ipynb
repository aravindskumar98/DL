{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoftMax and Cross Entropy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQVvYJo6buE1zbS99zVWp8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravindskumar98/DL/blob/main/SoftMax_and_Cross_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr2TYAdoUex6",
        "outputId": "10010c82-b4ab-45bf-cc49-905fbb6720f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax np: [0.65900114 0.24243297 0.09856589]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "##in numpy\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
        "\n",
        "x = np.array([2,1,0.1])\n",
        "outputs = softmax(x)\n",
        "\n",
        "print(\"softmax np:\", outputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## in torch\n",
        "x = torch.tensor([2.0,1.0,0.1])\n",
        "outputs = torch.softmax(x, dim = 0) ##dimension should be specified, so that it computes along first axis\n",
        "print(\"softmax np:\", outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uZw92p5vlxX",
        "outputId": "60533c46-0fcc-4d78-be29-c7b7007007b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax np: tensor([0.6590, 0.2424, 0.0986])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cross Entropy Loss = -1/n * sum(yi * log(yi_pred))\n",
        "'''\n",
        "This measures performance when output is between 0 and 1. Can be  used in multiclass problems also\n",
        "Loss increases as predicted probability diverges from label value\n",
        "'''"
      ],
      "metadata": {
        "id": "rfCnzuN3vw0G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## cross entropy loss in numnpy\n",
        "\n",
        "def cross_entropy(actual, predicted):\n",
        "  loss = -np.sum(actual*np.log(predicted))\n",
        "  return loss\n",
        "\n",
        "## y must be one hot encoded\n",
        "## i.e if class 0 --> [1 0 0]\n",
        "\n",
        "Y = np.array([1,0,0])\n",
        "\n",
        "y_pred_good = np.array([0.7,0.2,0.1])\n",
        "y_pred_bad = np.array([0.1,0.3,0.6])\n",
        "l1 = cross_entropy(Y,y_pred_good)\n",
        "l2 = cross_entropy(Y,y_pred_bad)\n",
        "print(\"Good\",l1,\"Bad\",l2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuymVNNOwTTa",
        "outputId": "4a1cfd29-2b7f-4bd8-8f1f-248b805f143c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good 0.35667494393873245 Bad 2.3025850929940455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Same in Pytorch\n",
        "'''\n",
        "crossentropyloss already applied applies logsoftmax and then negativeloglikelihood loss NLLLoss\n",
        "So we should not implement the softmax layer ourselves\n",
        "Y must not be one hot encoded. Correct class labels must be present\n",
        "Y_pred has raw scores, no softmax!\n",
        "\n",
        "'''\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "Y = torch.tensor([0])\n",
        "## nsamples X nclasses --> 1 x 3\n",
        "y_pred_good = torch.tensor([[2.0,1.0,0.1]]) ## array inside array\n",
        "y_pred_bad = torch.tensor([[0.5,2.0,0.3]]) ## array inside array\n",
        "\n",
        "l1 = loss(y_pred_good,Y)\n",
        "l2 = loss(y_pred_bad,Y)\n",
        "\n",
        "print(l1,l2) ## good has lower cross entropy loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0KS7uJ2wmyc",
        "outputId": "10b2cb3b-5073-4b53-99c6-38556c257d4d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4170) tensor(1.8406)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_,predictions1 = torch.max(y_pred_good,1) # 1 stands for along first dimension\n",
        "_,predictions2 = torch.max(y_pred_bad,1) # 1 stands for along first dimension\n",
        "print(predictions1.item(), predictions2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooZDfNQ6zqNK",
        "outputId": "76e018b9-e3b5-4779-a379-f466a6dec1f9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##increase number of samples\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "## 3 samples\n",
        "Y = torch.tensor([2,0,1])\n",
        "## nsamples X nclasses --> 3 x 3\n",
        "y_pred_good = torch.tensor([[2.0,1.0,4],[2.0,1.0,0.1],[2.0,3.0,0.1]]) ## array inside array\n",
        "y_pred_bad = torch.tensor([[5.5,2.0,0.3],[2.0,1.0,0.1],[2.0,1.0,0.1]]) ## array inside array\n",
        "\n",
        "l1 = loss(y_pred_good,Y)\n",
        "l2 = loss(y_pred_bad,Y)\n",
        "\n",
        "print(l1,l2) ## good has lower cross entropy loss\n",
        "\n",
        "_,predictions1 = torch.max(y_pred_good,1) # 1 stands for along first dimension\n",
        "_,predictions2 = torch.max(y_pred_bad,1) # 1 stands for along first dimension\n",
        "print(predictions1, predictions2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0iC2JzV0Bc0",
        "outputId": "1bf323bc-984f-4368-fd85-8cb6a584524a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3132) tensor(2.3564)\n",
            "tensor([2, 0, 1]) tensor([0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## NN with Softmax\n",
        "\n",
        "##since we use cross entropy loss, do not use softmax at the end!!\n",
        "\n",
        "## MULTI CLASS CLASSIFICATION\n",
        "\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,num_classes):\n",
        "    super(NeuralNet2,self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size,num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    ## no softmax at the end\n",
        "    return out\n",
        "\n",
        "model = NeuralNet2(input_size = 28*28, hidden_size = 5, num_classes = 3)\n",
        "criterion = nn.CrossEntropyLoss() ## applies Softmax\n",
        "\n"
      ],
      "metadata": {
        "id": "W0lTUPhg0jJJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BINARY CLASSIFICATION\n",
        "\n",
        "class NeuralNet1(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size):\n",
        "    super(NeuralNet1,self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    ## sigmoid at the end!! ---> important\n",
        "    y_pred = torch.sigmoid(out)\n",
        "    return y_pred\n",
        "\n",
        "model = NeuralNet2(input_size = 28*28, hidden_size = 5)\n",
        "criterion = nn.BCELoss() ## applies Softmax\n"
      ],
      "metadata": {
        "id": "0yu6TSkr3R3i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}