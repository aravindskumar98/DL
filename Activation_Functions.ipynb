{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation_Functions.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNzriODwbUohLkyGy2DCSz4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravindskumar98/DL/blob/main/Activation_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL_6zQv44j3_"
      },
      "outputs": [],
      "source": [
        "## apply a nonlinear transformation and decide whether a neuron should be activated or not\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. Step Function\n",
        "\n",
        "Not used in function\n",
        "\n",
        "2. Sigmoid Function\n",
        "\n",
        "f(x) = 1 / (1+e**-x)\n",
        "\n",
        "Used in last layer of binary classification\n",
        "\n",
        "3. TanH \n",
        "\n",
        "It is a scaled and shifted Sigmoid function\n",
        "Value between -1 and +1\n",
        "A very good choice in hidden layers\n",
        "\n",
        "4. ReLU\n",
        "\n",
        "Most popular choice in networks\n",
        "f(x) = max(0,x)\n",
        "If you dont know what to use, use ReLU!!\n",
        "\n",
        "5. Leaky Relu\n",
        "f(x) = x if x > 0 else a.x  ---------> a < 1\n",
        "This solves the vanishing gradient problem in RelU ---> Some weights arent updated because gradient is zero on LHS\n",
        "\n",
        "6. SoftMax\n",
        "watch prev implementations\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "xecBFNPi4-Ap",
        "outputId": "55bbd0cd-809e-47fc-e7ba-6e3ad0b189b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n1. Step Function\\n\\nNot used in function\\n\\n2. Sigmoid Function\\n\\nf(x) = 1 / (1+e**-x)\\n\\nUsed in last layer of binary classification\\n\\n3. TanH \\n\\nIt is a scaled and shifted Sigmoid function\\nValue between -1 and +1\\nA very good choice in hidden layers\\n\\n4. ReLU\\n\\nMost popular choice in networks\\nf(x) = max(0,x)\\nIf you dont know what to use, use ReLU!!\\n\\n5. Leaky Relu\\nf(x) = x if x > 0 else a.x  ---------> a < 1\\nThis solves the vanishing gradient problem in RelU ---> Some weights arent updated because gradient is zero on LHS\\n\\n6. SoftMax\\nwatch prev implementations\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## IMPLEMENTATION ---> two ways of implementing\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "## 1 --> create nn modules\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size):\n",
        "    super(NeuralNet,self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "## 2 --> use activation functions directly in forward pass\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size):\n",
        "    super(NeuralNet,self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size,hidden_size)\n",
        "    # self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size,1)\n",
        "    # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.relu(self.linear1(x))\n",
        "    out = torch.sigmoid(self.linear2(out))\n",
        "    return out\n",
        "\n",
        "# ALso check out this library \n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# LeakyReLU is available in F similar to how it is in torch. | it is available for use as in method 1 in nn also\n",
        "\n"
      ],
      "metadata": {
        "id": "zIsB7_Uk6BvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}